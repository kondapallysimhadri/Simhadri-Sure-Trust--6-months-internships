{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e7d71a",
   "metadata": {},
   "source": [
    "commann code on the worksheet in RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1c0d015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all common on the setup in modules\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state, info = env.reset()\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "386cfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Sequential decision process\n",
    "trajectories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4587868f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(2),\n",
       " Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 MDP components\n",
    "S = env.observation_space\n",
    "A = env.action_space\n",
    "\n",
    "A,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f26a5e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 Markov property (depends only on current state)\n",
    "action = A.sample()\n",
    "next_state, reward, terminated, trucated, info = env.step(action)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bddc5d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03858292, -0.00603979,  0.00146659, -0.0433813 ], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 State representation\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "312cb751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 Real-world MDP example (cart-pole environment)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a4f4a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03858292 -0.00603979  0.00146659 -0.0433813 ]\n",
      "[-0.03858292 -0.00603979  0.00146659 -0.0433813 ]\n"
     ]
    }
   ],
   "source": [
    "#6 State vs observation\n",
    "observation = state\n",
    "print(observation)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "634c935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 Action definition\n",
    "action = A.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "abe21d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03858292, -0.00603979,  0.00146659, -0.0433813 ], dtype=float32),\n",
       " 1,\n",
       " 1.0,\n",
       " array([-0.03870372,  0.1890611 ,  0.00059896, -0.33560115], dtype=float32))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8 Transition probability (implicit in env)\n",
    "transition = (state, action, reward, next_state)\n",
    "transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0c438505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.0349225 ,  0.38417453, -0.00611306, -0.62809515], dtype=float32),\n",
       " array([-0.02723901,  0.18913841, -0.01867496, -0.33734366], dtype=float32),\n",
       " array([-0.02345624,  0.38452107, -0.02542184, -0.6358567 ], dtype=float32),\n",
       " array([-0.01576582,  0.18976274, -0.03813897, -0.35128677], dtype=float32),\n",
       " array([-0.01197056,  0.38540572, -0.0451647 , -0.65574795], dtype=float32)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9 Stochastic transition (noise effect)\n",
    "state = [env.step(A.sample())[0] for _ in range(5)]\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b53d97df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 reward function\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c1028e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#11 Negative reward example\n",
    "penalty = -10\n",
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8e25aa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#12 Policy definition\n",
    "def policy(S):\n",
    "    return A.sample()\n",
    "\n",
    "policy(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "aa6d9d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#13 Deterministic policy\n",
    "def deterministic_policy(S):\n",
    "    return 0\n",
    "deterministic_policy(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0651a70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14 Stochastic policy\n",
    "def stochastic_policy(S):\n",
    "    return np.random.choice([0,1],p=[0.7, 0.3])\n",
    "stochastic_policy(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "04b4a4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.99, 2.9701, 3.940399, 4.90099501]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#15 Return calculation\n",
    "returns = []\n",
    "G = 0\n",
    "for t in range(5):\n",
    "    G += (gamma ** t) * reward\n",
    "    returns.append(G)\n",
    "    \n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "81a919d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#16 Discount factor usage\n",
    "discounted_rewards = reward * gamma\n",
    "discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe71aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
